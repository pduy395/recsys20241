{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', dtype={'article_id':str})\n",
    "transactions.drop(['sales_channel_id', 'price'], inplace=True, axis=1)\n",
    "transactions['bought'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = datetime.datetime(2020,9,1)\n",
    "# Filter transactions by date\n",
    "transactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\n",
    "# transactions = transactions.loc[transactions[\"t_dat\"] >= start_date]\n",
    "\n",
    "# Filter transactions by number of an article has been bought\n",
    "article_bought_count = transactions[['article_id', 't_dat']].groupby('article_id').count().reset_index().rename(columns={'t_dat': 'count'})\n",
    "most_bought_articles = article_bought_count[article_bought_count['count']>10]['article_id'].values\n",
    "transactions = transactions[transactions['article_id'].isin(most_bought_articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative samples\n",
    "np.random.seed(0)\n",
    "\n",
    "negative_samples = pd.DataFrame({\n",
    "    'article_id': np.random.choice(transactions.article_id.unique(), transactions.shape[0]),\n",
    "    'customer_id': np.random.choice(transactions.customer_id.unique(), transactions.shape[0]),\n",
    "    'bought': np.zeros(transactions.shape[0])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class ItemBased_RecSys:\n",
    "    ''' Collaborative filtering using a custom sim(u,u'). '''\n",
    "\n",
    "    def __init__(self, positive_transactions, negative_transactions, num_components=10):\n",
    "        ''' Constructor '''\n",
    "        self.positive_transactions = positive_transactions\n",
    "        self.transactions = pd.concat([positive_transactions, negative_transactions])\n",
    "        self.customers = self.transactions.customer_id.values\n",
    "        self.articles = self.transactions.article_id.values\n",
    "        self.bought = self.transactions.bought.values\n",
    "        self.num_components = num_components\n",
    "\n",
    "        self.customer_id2index = {c: i for i, c in enumerate(np.unique(self.customers))}\n",
    "        self.article_id2index = {a: i for i, a in enumerate(np.unique(self.articles))}\n",
    "        \n",
    "    def __sdg__(self):\n",
    "        for idx in tqdm(self.training_indices):\n",
    "            # Get the current sample\n",
    "            customer_id = self.customers[idx]\n",
    "            article_id = self.articles[idx]\n",
    "            bought = self.bought[idx]\n",
    "\n",
    "            # Get the index of the user and the article\n",
    "            customer_index = self.customer_id2index[customer_id]\n",
    "            article_index = self.article_id2index[article_id]\n",
    "\n",
    "            # Compute the prediction and the error\n",
    "            prediction = self.predict_single(customer_index, article_index)\n",
    "            error = (bought - prediction) # error\n",
    "            \n",
    "            # Update latent factors in terms of the learning rate and the observed error\n",
    "            self.customers_latent_matrix[customer_index] += self.learning_rate * \\\n",
    "                                    (error * self.articles_latent_matrix[article_index] - \\\n",
    "                                     self.lmbda * self.customers_latent_matrix[customer_index])\n",
    "            self.articles_latent_matrix[article_index] += self.learning_rate * \\\n",
    "                                    (error * self.customers_latent_matrix[customer_index] - \\\n",
    "                                     self.lmbda * self.articles_latent_matrix[article_index])\n",
    "                \n",
    "                \n",
    "    def fit(self, n_epochs=10, learning_rate=0.001, lmbda=0.1):\n",
    "        ''' Compute the matrix factorization R = P x Q '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lmbda = lmbda\n",
    "        n_samples = self.transactions.shape[0]\n",
    "        \n",
    "        # Initialize latent matrices\n",
    "        self.customers_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(self.customers)), self.num_components))\n",
    "        self.articles_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(self.articles)), self.num_components))\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            print('Epoch: {}'.format(epoch))\n",
    "            self.training_indices = np.arange(n_samples)\n",
    "            \n",
    "            # Shuffle training samples and follow stochastic gradient descent\n",
    "            np.random.shuffle(self.training_indices)\n",
    "            self.__sdg__()\n",
    "\n",
    "    def predict_single(self, customer_index, article_index):\n",
    "        ''' Make a prediction for an specific user and article '''\n",
    "        prediction = np.dot(self.customers_latent_matrix[customer_index], self.articles_latent_matrix[article_index])\n",
    "        prediction = np.clip(prediction, 0, 1)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    def default_recommendation(self):\n",
    "        ''' Calculate time decaying popularity '''\n",
    "        # Calculate time decaying popularity. This leads to items bought more recently having more weight in the popularity list.\n",
    "        # In simple words, item A bought 5 times on the first day of the train period is inferior than item B bought 4 times on the last day of the train period.\n",
    "        self.positive_transactions['pop_factor'] = self.positive_transactions['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,23) - x).days)\n",
    "        transactions_by_article = self.positive_transactions[['article_id', 'pop_factor']].groupby('article_id').sum().reset_index()\n",
    "        return transactions_by_article.sort_values(by='pop_factor', ascending=False)['article_id'].values[:12]\n",
    "\n",
    "\n",
    "    def predict(self, customers):\n",
    "        ''' Make recommendations '''\n",
    "        recommendations = []\n",
    "\n",
    "        # Compute similarity matrix (cosine)\n",
    "        similarity_matrix = cosine_similarity(self.articles_latent_matrix, self.articles_latent_matrix, dense_output=False)\n",
    "\n",
    "        # Convert similarity matrix into a matrix containing the 12 most similar items' index for each item\n",
    "        similarity_matrix = np.argsort(similarity_matrix, axis=1)\n",
    "        similarity_matrix = similarity_matrix[:, -12:]\n",
    "\n",
    "        # Get default recommendation (time decay popularity)\n",
    "        default_recommendation = self.default_recommendation()\n",
    "\n",
    "        # Group articles by user and articles to compute the number of times each article has been bought by each user\n",
    "        transactions_by_customer = self.positive_transactions[['customer_id', 'article_id', 'bought']].groupby(['customer_id', 'article_id']).count().reset_index()\n",
    "        most_bought_article = transactions_by_customer.loc[transactions_by_customer.groupby('customer_id').bought.idxmax()]['article_id'].values\n",
    "\n",
    "        # Make predictions\n",
    "        for customer in tqdm(customers):\n",
    "            try:\n",
    "                rec_aux1 = []\n",
    "                rec_aux2 = []\n",
    "                aux = []\n",
    "\n",
    "                # Retrieve the most bought article by customer\n",
    "                user_most_bought_article_id = most_bought_article[self.customer_id2index[customer]]\n",
    "\n",
    "                # Using the similarity matrix, get the 6 most similar articles\n",
    "                rec_aux1 = self.articles[similarity_matrix[self.article_id2index[user_most_bought_article_id]]]\n",
    "                # Return the half of the default recommendation\n",
    "                rec_aux2 = default_recommendation\n",
    "\n",
    "                # Merge half of both recommendation lists\n",
    "                for rec_idx in range(6):\n",
    "                    aux.append(rec_aux2[rec_idx])\n",
    "                    aux.append(rec_aux1[rec_idx])\n",
    "\n",
    "                recommendations.append(' '.join(aux))\n",
    "            except:\n",
    "                # Return the default recommendation\n",
    "                recommendations.append(' '.join(default_recommendation))\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'customer_id': customers,\n",
    "            'prediction': recommendations,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = ItemBased_RecSys(transactions, negative_samples, num_components=1000)\n",
    "rec.fit(n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv').customer_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = rec.predict(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
